<!DOCTYPE html>
<html class="no-js" lang="zh">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="lt's notes" name="description"/>
<link href="https://626lt.github.io/read/diffusion/" rel="canonical"/>
<link href="../" rel="prev"/>
<link href="../Consistency%20Models/" rel="next"/>
<link href="../../assets/images/favicon.png" rel="icon"/>
<meta content="mkdocs-1.6.1, mkdocs-material-9.0.4" name="generator"/>
<title>Diffusion - lt's notes</title>
<link href="../../assets/stylesheets/main.9c788c91.min.css" rel="stylesheet"/>
<link href="../../assets/stylesheets/palette.2505c338.min.css" rel="stylesheet"/>
<link href="../../css/heti.css" rel="stylesheet"/>
<link href="https://cdn.tonycrane.cc/utils/katex.min.css" rel="stylesheet"/>
<link href="../../css/custom.css" rel="stylesheet"/>
<link href="../../css/counter.css" rel="stylesheet"/>
<link href="https://gcore.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" rel="stylesheet"/>
<link href="https://gcore.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css" rel="stylesheet"/>
<link href="https://gcore.jsdelivr.net/npm/lxgw-wenkai-webfont@1.1.0/style.css" rel="stylesheet"/>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
</head>
<body data-md-color-accent="" data-md-color-primary="" data-md-color-scheme="default" dir="ltr">
<script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#diffusion">
          跳转至
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="页眉" class="md-header__inner md-grid">
<a aria-label="lt's notes" class="md-header__button md-logo" data-md-component="logo" href="../.." title="lt's notes">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 4v6l-2-2-2 2V4H9v16h10V4h-2M3 7V5h2V4a2 2 0 0 1 2-2h12c1.05 0 2 .95 2 2v16c0 1.05-.95 2-2 2H7c-1.05 0-2-.95-2-2v-1H3v-2h2v-4H3v-2h2V7H3m2-2v2h2V5H5m0 14h2v-2H5v2m0-6h2v-2H5v2Z"></path></svg>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            lt's notes
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Diffusion
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="夜间模式" class="md-option" data-md-color-accent="" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="" data-md-color-scheme="default" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_2" hidden="" title="夜间模式">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"></path></svg>
</label>
<input aria-label="日间模式" class="md-option" data-md-color-accent="" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="teal" data-md-color-scheme="slate" id="__palette_2" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="日间模式">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"></path></svg>
</label>
</form>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="搜索" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="搜索" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</label>
<nav aria-label="查找" class="md-search__options">
<button aria-label="清空当前内容" class="md-search__icon md-icon" tabindex="-1" title="清空当前内容" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
<ol class="md-search-result__list"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/626lt/note" title="前往仓库">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
</div>
<div class="md-source__repository">
    626lt/note
  </div>
</a>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<nav aria-label="标签" class="md-tabs" data-md-component="tabs">
<div class="md-tabs__inner md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../..">
      Home
    </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../cg/">
        CG
      </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../os/">
        OS
      </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../ml/">
        ML
      </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link md-tabs__link--active" href="../">
        Read
      </a>
</li>
</ul>
</div>
</nav>
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="导航栏" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="lt's notes" class="md-nav__button md-logo" data-md-component="logo" href="../.." title="lt's notes">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 4v6l-2-2-2 2V4H9v16h10V4h-2M3 7V5h2V4a2 2 0 0 1 2-2h12c1.05 0 2 .95 2 2v16c0 1.05-.95 2-2 2H7c-1.05 0-2-.95-2-2v-1H3v-2h2v-4H3v-2h2V7H3m2-2v2h2V5H5m0 14h2v-2H5v2m0-6h2v-2H5v2Z"></path></svg>
</a>
    lt's notes
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/626lt/note" title="前往仓库">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
</div>
<div class="md-source__repository">
    626lt/note
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../..">
        Home
      </a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" id="__nav_2" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../cg/">CG</a>
<label for="__nav_2">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="CG" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_2">
<span class="md-nav__icon md-icon"></span>
          CG
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../cg/Intro/">
        Introduction
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../cg/2D%20Graphics/">
        2D Graphics
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../cg/OpenGL/">
        OpenGL
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../cg/Geometric%20Transformation/">
        Geometric Transformation
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../cg/View/">
        View
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../cg/Antialiasing/">
        Antialiasing
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" id="__nav_3" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../os/">OS</a>
<label for="__nav_3">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="OS" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
          OS
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../os/intro/">
        intro
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../os/structures/">
        structures
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../os/Threads/">
        Threads
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../os/Synchronization/">
        Synchronization
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../os/Deadlock/">
        Deadlock
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" id="__nav_4" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../ml/">ML</a>
<label for="__nav_4">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="ML" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_4">
<span class="md-nav__icon md-icon"></span>
          ML
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../ml/probability%20graph/">
        probability graph
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" id="__nav_5" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../">Read</a>
<label for="__nav_5">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="Read" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_5">
<span class="md-nav__icon md-icon"></span>
          Read
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" data-md-toggle="toc" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
          Diffusion
          <span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
        Diffusion
      </a>
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      目录
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#latent">
    Latent
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#elbo">
    ELBO
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#variational-autoencoder-vae">
    Variational Autoencoder (VAE)
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#hierarchical-vae">
    Hierarchical VAE
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#variational-diffusion-models">
    Variational Diffusion Models
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#learning-diffusion-noise-parameters">
    Learning Diffusion Noise parameters
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#three-equivalent-interpretations">
    Three Equivalent Interpretations
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#score-based-generative-modeling">
    Score-based Generative Modeling
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#guidance">
    Guidance
  </a>
<nav aria-label="Guidance" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#classifier-guidance">
    Classifier Guidance
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#classifier-free-guidance">
    Classifier-Free Guidance
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#summary">
    Summary
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../Consistency%20Models/">
        Consistency Models
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../SDE-diffusion/">
        SDE-Diffusion
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      目录
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#latent">
    Latent
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#elbo">
    ELBO
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#variational-autoencoder-vae">
    Variational Autoencoder (VAE)
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#hierarchical-vae">
    Hierarchical VAE
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#variational-diffusion-models">
    Variational Diffusion Models
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#learning-diffusion-noise-parameters">
    Learning Diffusion Noise parameters
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#three-equivalent-interpretations">
    Three Equivalent Interpretations
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#score-based-generative-modeling">
    Score-based Generative Modeling
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#guidance">
    Guidance
  </a>
<nav aria-label="Guidance" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#classifier-guidance">
    Classifier Guidance
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#classifier-free-guidance">
    Classifier-Free Guidance
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#summary">
    Summary
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<h1 id="diffusion">Diffusion<a class="headerlink" href="#diffusion" title="Permanent link">¶</a></h1>
<div style="margin-top: -30px; font-size: 0.75em; opacity: 0.7;">
<p><span class="twemoji"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10h-2a8 8 0 0 1-8 8 8 8 0 0 1-8-8 8 8 0 0 1 8-8V2m6.78 1a.69.69 0 0 0-.48.2l-1.22 1.21 2.5 2.5L20.8 5.7c.26-.26.26-.7 0-.95L19.25 3.2c-.13-.13-.3-.2-.47-.2m-2.41 2.12L9 12.5V15h2.5l7.37-7.38-2.5-2.5Z"></path></svg></span> 约<span class="heti-skip"><span class="heti-spacing"> </span>9864<span class="heti-spacing"> </span></span>个字 <span class="twemoji"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 20c4.42 0 8-3.58 8-8s-3.58-8-8-8-8 3.58-8 8 3.58 8 8 8m0-18c5.5 0 10 4.5 10 10s-4.5 10-10 10C6.47 22 2 17.5 2 12S6.5 2 12 2m.5 11H11V7h1.5v4.26l3.7-2.13.75 1.3L12.5 13Z"></path></svg></span> 预计阅读时间<span class="heti-skip"><span class="heti-spacing"> </span>33<span class="heti-spacing"> </span></span>分钟</p>
</div>
<h2 id="latent">Latent<a class="headerlink" href="#latent" title="Permanent link">¶</a></h2>
<p>base assumption: the data/knowledge we learned can be represented by a high/low-dimensional latent space.
尽管这种变量不能被直观的观察到，我们仍可以描述和从中推断。</p>
<p>在尝试建模<span class="heti-skip"><span class="heti-spacing"> </span>latent<span class="heti-spacing"> </span></span>的过程中，存在建模高维还是低维数据的选择。大多数偏向其是低维的建模，原因是这是因为如果没有强大的先验知识，试图学习比观察更高维度的表示是徒劳的，另一方面是因为低维数据可以看作是一种压缩的表示，并且可以潜在地揭示描述观察结果的语义上有意义的结构。</p>
<h2 id="elbo">ELBO<a class="headerlink" href="#elbo" title="Permanent link">¶</a></h2>
<p><span>ELBO means Evidence Lower Bound. It is a<span class="heti-spacing"> </span></span><strong><span>lower bound<span class="heti-spacing"> </span></span></strong><span>of the log likelihood of the data. It is used in variational inference to approximate the posterior distribution.<span class="heti-spacing"> </span></span>对<span class="heti-skip"><span class="heti-spacing"> </span>ELBO<span class="heti-spacing"> </span></span>的合理参数化和优化可以逼近实际的后验分布。</p>
<p>形式上定义为：</p>
<div class="arithmatex">\[
\begin{aligned}
\text{ELBO}(\theta, \phi) = \mathbb{E}_{q_{\phi}(z|x)}[\log \frac{p_{\theta}(x, z)}{q_{\phi}(z|x)}]
\end{aligned}
\]</div>
<p>这里的 <span class="arithmatex">\(p_{\theta}(x, z)\)</span> 是<span class="heti-skip"><span class="heti-spacing"> </span>latent variable<span class="heti-spacing"> </span></span>与<span class="heti-skip"><span class="heti-spacing"> </span>data<span class="heti-spacing"> </span></span>的联合分布。 <span class="arithmatex">\(q_{\phi}(z|x)\)</span> 是<span><span class="heti-spacing"> </span>flexible approximate variational distribution</span>，由 <span class="arithmatex">\(\phi\)</span> 参数化以寻求优化。</p>
<p><span>ELBO<span class="heti-spacing"> </span></span>可以看作是一个参数化的模型去评估在给定<span class="heti-skip"><span class="heti-spacing"> </span>observation<span class="heti-spacing"> </span></span><span class="arithmatex">\(x\)</span>下的<span class="heti-skip"><span class="heti-spacing"> </span>latent variable<span class="heti-spacing"> </span></span><span class="arithmatex">\(z\)</span>的分布。也就是说，它试图逼近真实的后验 <span class="arithmatex">\(p(z|x)\)</span>。当这个逼近足够好的时候，我们就获得了真实数据的分布，然后就可以做各种推理来生成。</p>
<p>下面我们从数学角度来说明为什么<span class="heti-skip"><span class="heti-spacing"> </span>ELBO<span class="heti-spacing"> </span></span>是一个合适的优化对象。</p>
<div class="arithmatex">\[
\begin{aligned}
\log p(x) &amp;= \log \int p(x, z) dz \\
&amp;= \log \int q_\phi(z|x) \frac{p(x, z)}{q_\phi(z|x)} dz \\
&amp;= \log \mathbb{E}_{q_\phi(z|x)} \left[ \frac{p(x, z)}{q_\phi(z|x)} \right] \\
&amp;\geq \mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p(x, z)}{q_\phi(z|x)} \right] \\
\end{aligned}
\]</div>
<p>最后一步是由<span class="heti-skip"><span class="heti-spacing"> </span>Jensen<span class="heti-spacing"> </span></span>不等式得到的。这里的 <span class="arithmatex">\(\mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p(x, z)}{q_\phi(z|x)} \right]\)</span> 就是<span><span class="heti-spacing"> </span>ELBO</span>。</p>
<div class="admonition note 期望">
<p class="admonition-title">Note</p>
<p>随机变量<span class="arithmatex">\(Z\)</span> 在分布 <span class="arithmatex">\(q(z)\)</span> 下的期望表示为 <span class="arithmatex">\(E_{q(z)}[f(Z)]=\int f(Z)q(z) dz\)</span></p>
</div>
<p>但是通过<span class="heti-skip"><span class="heti-spacing"> </span>Jensen<span class="heti-spacing"> </span></span>不等式只是说明了<span class="heti-skip"><span class="heti-spacing"> </span>ELBO<span class="heti-spacing"> </span></span>是一个下界，但是没有说明为什么这个下界可以达到，适合去优化。于是我们可以通过<span class="heti-skip"><span class="heti-spacing"> </span>KL<span class="heti-spacing"> </span></span>散度来证明这一点。</p>
<div class="admonition note kl 散度">
<p class="admonition-title">Note</p>
<p><span>KL<span class="heti-spacing"> </span></span>散度是两个分布之间的差异度量。对于两个分布 <span class="arithmatex">\(p(x)\)</span> 和 <span class="arithmatex">\(q(x)\)</span>，<span>KL<span class="heti-spacing"> </span></span>散度定义为：
$$
D_\text{KL}(p||q) = \int p(x) \log \frac{p(x)}{q(x)} dx = E_{p(x)}[\log \frac{p(x)}{q(x)}]
$$</p>
</div>
<div class="arithmatex">\[
\begin{aligned}
\log p(x) &amp;= \log p(x) \int q_\phi(z|x) dz \\
&amp;= \int q_\phi(z|x) \log p(x) dz \\
&amp;= \mathbb{E}_{q_\phi(z|x)}[\log p(x)] \\
&amp;= \mathbb{E}_{q_\phi(z|x)}[\log \frac{p(x, z)}{p(z|x)}] \\
&amp;= \mathbb{E}_{q_\phi(z|x)}[\log \frac{p(x, z)}{q_\phi(z|x)}] + \mathbb{E}_{q_\phi(z|x)}[\log \frac{q_\phi(z|x)}{p(z|x)}] \\
&amp;= \mathbb{E}_{q_\phi(z|x)}[\log \frac{p(x, z)}{q_\phi(z|x)}] + D_\text{KL}(q_\phi(z|x) || p(z|x)) \\
&amp;= \text{ELBO}(\theta, \phi) + D_\text{KL}(q_\phi(z|x) || p(z|x))\\
&amp;\geq \text{ELBO}(\theta, \phi)
\end{aligned}
\]</div>
<p>在上面的推导中我们可以看到<span class="heti-skip"><span class="heti-spacing"> </span>KL<span class="heti-spacing"> </span></span>散度就是<span class="heti-skip"><span class="heti-spacing"> </span>evidence<span class="heti-spacing"> </span></span>与<span class="heti-skip"><span class="heti-spacing"> </span>ELBO<span class="heti-spacing"> </span></span>的差值，这说明<span class="heti-skip"><span class="heti-spacing"> </span>ELBO<span class="heti-spacing"> </span></span>确实是一个下界</p>
<p></p><center><img src="./figures/2024-10-27-21-59-10.png" width="60%"/></center>
<p>这里我们希望得到的是真实的分布 <span class="arithmatex">\(p(z|x)\)</span>，但是由于这个分布是不可解的，我们希望通过 <span class="arithmatex">\(q_\phi(z|x)\)</span>来近似真实分布，这通过最小化<span class="heti-skip"><span class="heti-spacing"> </span>KL<span class="heti-spacing"> </span></span>散度来实现，但很难直接去做最小化。同时注意到的是我们的证据即对数似然是一个与 <span class="arithmatex">\(\phi\)</span> 无关的常数，所以通过<span class="arithmatex">\(\phi\)</span>的最大化<span class="heti-skip"><span class="heti-spacing"> </span>ELBO<span class="heti-spacing"> </span></span>可以起到同样的效果。而当<span class="heti-skip"><span class="heti-spacing"> </span>ELBO<span class="heti-spacing"> </span></span>足够大的时候，我们就可以认为 <span class="arithmatex">\(q_\phi(z|x)\)</span> 逼近了真实的分布 <span class="arithmatex">\(p(z|x)\)</span>。那么一旦得到了这个分布以后，我们就可以从中做推断，通过解码器 <span class="arithmatex">\(p_\theta(x|z)\)</span> 来生成数据。</p>
<h2 id="variational-autoencoder-vae">Variational Autoencoder (VAE)<a class="headerlink" href="#variational-autoencoder-vae" title="Permanent link">¶</a></h2>
<p>在<span class="heti-skip"><span class="heti-spacing"> </span>default formulation of VAE,<span class="heti-spacing"> </span></span>我们直接最大化<span><span class="heti-spacing"> </span>ELBO.</span>
+ variational，因为我们在由 <span class="arithmatex">\(\phi\)</span> 参数化的一系列潜在后验分布中寻找最优的参数 <span class="arithmatex">\(\phi\)</span> 来优化 <span class="arithmatex">\(q_\phi(z|x)\)</span>。
+ autoencoder，因为输入数据在经历中间瓶颈表示步骤后被训练以预测自身</p>
<p>下面来进一步分析<span><span class="heti-spacing"> </span>ELBO</span>：</p>
<div class="arithmatex">\[
\begin{aligned}
\text{ELBO}(\theta, \phi) &amp;= \mathbb{E}_{q_{\phi}(z|x)}[\log \frac{p_{\theta}(x, z)}{q_{\phi}(z|x)}] \\
&amp;= \mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)] + \mathbb{E}_{q_{\phi}(z|x)}[\log \frac{p_{\theta}(z)}{q_{\phi}(z|x)}] \\
&amp;= \mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)] - D_\text{KL}(q_{\phi}(z|x) || p_{\theta}(z))
\end{aligned}
\]</div>
<p>在这里，<span class="arithmatex">\(q_\phi(z|x)\)</span> 被视作<span><span class="heti-spacing"> </span>encoder</span>，<span class="arithmatex">\(p_\theta(x|z)\)</span> 被视作<span><span class="heti-spacing"> </span>decoder</span>。</p>
<p><span>the first term measures the reconstruction likelihood of the decoder from our variational distribution; this ensures that the learned distribution is modeling effective latents that the original data can be regenerated from.<span class="heti-spacing"> </span></span>第一项衡量的是变分分布测量解码器的重建可能性。这确保了学到的分布是在建模有效的潜在变量，原始数据可以从中再生。
The second term measures how similar the learned variational distribution is to a prior belief held over latent variables. 第二项衡量的是学到的分布与实际的分布（prior belief）的相似程度</p>
<p>从公式中我们可以看到，最大化<span class="heti-skip"><span class="heti-spacing"> </span>ELBO<span class="heti-spacing"> </span></span>的过程就是在最大化重建概率和最小化<span class="heti-skip"><span class="heti-spacing"> </span>KL<span class="heti-spacing"> </span></span>散度。</p>
<p><span>VAE<span class="heti-spacing"> </span></span>的一个决定性特征是如何对参数 <span class="arithmatex">\(\phi\)</span> 和 <span class="arithmatex">\(\theta\)</span> 联合优化<span><span class="heti-spacing"> </span>ELBO</span>。有以下的定义：</p>
<div class="arithmatex">\[
\begin{aligned}
q_\phi(z|x) &amp;= \mathcal{N}(z; \mu_\phi(x), \sigma^2_\phi(x)I) \\
p(z) &amp;= \mathcal{N}(z; 0, I) \\
\end{aligned}
\]</div>
<p>有了这两个定义，<span>KL<span class="heti-spacing"> </span></span>散度项就可以计算了，重建项可以用蒙特卡洛测量来估计。我们的目标可以重写为：</p>
<div class="arithmatex">\[
\begin{aligned}
&amp; \argmax_{\phi,\theta} \mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)] - D_\text{KL}(q_{\phi}(z|x) || p_{\theta}(z)) \\
\approx &amp; \argmax_{\phi,\theta} \frac{1}{L} \sum_{l=1}^L \log p_{\theta}(x|z^{(l)}) - D_\text{KL}(q_{\phi}(z|x) || p_{\theta}(z))
\end{aligned}
\]</div>
<p>这里的 <span class="arithmatex">\(z^{(l)}\)</span> 是从 <span class="arithmatex">\(q_\phi(z|x)\)</span> 中采样得到的，对于每一个<span class="heti-skip"><span class="heti-spacing"> </span>obserbation<span class="heti-spacing"> </span></span><span class="arithmatex">\(x\)</span>都会采样 <span class="arithmatex">\(L\)</span> 个 <span class="arithmatex">\(z\)</span>。然而这样的默认设置带来问题：计算损失的每个<span class="heti-skip"><span class="heti-spacing"> </span>z(l)<span class="heti-spacing"> </span></span>都是由随机采样过程生成的，该过程通常是不可微分的。为了解决这个问题，我们可以使用重参数化技巧当 <span class="arithmatex">\(q_\phi(z|x)\)</span> 被设计为对某些分布建模时。</p>
<div class="admonition note reparameterization trick">
<p class="admonition-title">Note</p>
<p>这个重参数方法是通过引入一个额外的噪声项 <span class="arithmatex">\(\epsilon\)</span> 来实现的，将随机变量转化为噪声的确定函数，这就允许我们通过梯度下降来优化非随机项。例如从正态分布 <span class="arithmatex">\(x \sim \mathcal{N}(\mu, \sigma^2)\)</span> 中采样，我们可以通过 <span class="arithmatex">\(x = \mu + \sigma \epsilon\)</span> 来实现，其中 <span class="arithmatex">\(\epsilon \sim \mathcal{N}(0, 1)\)</span>。因此，通过重新参数化技巧，可以通过从标准高斯分布采样、按目标标准差缩放结果并按目标均值平移结果来执行任意高斯分布的采样。</p>
</div>
<p>在<span class="heti-skip"><span class="heti-spacing"> </span>VAE<span class="heti-spacing"> </span></span>中，我们可以通过以下方式来重参数化：这里 <span class="arithmatex">\(z\)</span> 被定义为输入 <span class="arithmatex">\(x\)</span> 和辅助噪声变量 <span class="arithmatex">\(\epsilon\)</span>的确定性函数。</p>
<div class="arithmatex">\[
\begin{aligned}
z &amp;= \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon \\
\epsilon &amp;\sim \mathcal{N}(0, I)
\end{aligned}
\]</div>
<p>这里 <span class="arithmatex">\(\odot\)</span> 表示逐元素乘法。在这个表示下，可以根据需要计算梯度来优化 <span class="arithmatex">\(\mu\)</span> 和 <span class="arithmatex">\(\theta\)</span>。</p>
<p>在训练了<span class="heti-skip"><span class="heti-spacing"> </span>VAE<span class="heti-spacing"> </span></span>后，可以通过在潜在空间中采样 <span class="arithmatex">\(z\)</span> 并通过解码器 <span class="arithmatex">\(p_\theta(x|z)\)</span> 来生成新的数据。当<span class="heti-skip"><span class="heti-spacing"> </span>latent space<span class="heti-spacing"> </span></span>的维度较低时，可能会到学习紧凑、有用的表示。此外，当学习到语义上有意义的潜在空间时，可以在将潜在向量传递到解码器之前对其进行编辑，以更精确地控制生成的数据。</p>
<h2 id="hierarchical-vae">Hierarchical VAE<a class="headerlink" href="#hierarchical-vae" title="Permanent link">¶</a></h2>
<p><span>Hierarchical VAE<span class="heti-spacing"> </span></span>是<span class="heti-skip"><span class="heti-spacing"> </span>VAE<span class="heti-spacing"> </span></span>的一种扩展，它通过在潜在空间中引入多个层次的结构来提高建模能力。在这种结构中，每个层次的潜在变量都可以被视为对数据的不同抽象级别的表示，从而允许模型学习更复杂的数据分布。尽管对一般的<span class="heti-skip"><span class="heti-spacing"> </span>HVAE<span class="heti-spacing"> </span></span>来说，每个<span class="heti-skip"><span class="heti-spacing"> </span>latent variable<span class="heti-spacing"> </span></span>都可以以先前所有的<span class="heti-skip"><span class="heti-spacing"> </span>latent variable<span class="heti-spacing"> </span></span>为条件，但是在这里，我们考虑特殊情况——马尔可夫<span><span class="heti-spacing"> </span>VAE</span>（ Markovian HVAE (MHVAE)）。这说明每个<span class="heti-skip"><span class="heti-spacing"> </span>latent variable<span class="heti-spacing"> </span></span>只依赖于前一个<span><span class="heti-spacing"> </span>latent variable</span>。直观来说就是叠很多层的<span><span class="heti-spacing"> </span>VAE</span>，每一层的输出作为下一层的输入。</p>
<p></p><center><img src="./figures/2024-10-28-00-03-30.png" width="60%"/></center>
<p>数学上，<span>MHVAE<span class="heti-spacing"> </span></span>的表示为：</p>
<div class="arithmatex">\[  
\begin{aligned}
p(x,z_{1:T}) &amp;= p(z_T)p_\theta(x|z_1) \prod_{t=2}^T p(z_{t-1}|z_{t}) \\
q_\phi(z_{1:T}|x) &amp;= q_\phi(z_1|x) \prod_{t=2}^T q_\phi(z_t|z_{t-1})
\end{aligned}
\]</div>
<p>我们可以将<span class="heti-skip"><span class="heti-spacing"> </span>ELBO<span class="heti-spacing"> </span></span>推广为：</p>
<div class="arithmatex">\[
\begin{aligned}
\text{ELBO}(\theta, \phi) &amp;= \log \mathbb{E}_{q_{\phi}(z_{1:T}|x)}[\frac{p_{\theta}(x, z_{1:T})}{q_{\phi}(z_{1:T}|x)}] \\
&amp;\geq \mathbb{E}_{q_{\phi}(z_{1:T}|x)}[\log \frac{p_{\theta}(x, z_{1:T})}{q_{\phi}(z_{1:T}|x)}]
\end{aligned}
\]</div>
<p>然后把上面的定义带入到<span class="heti-skip"><span class="heti-spacing"> </span>ELBO<span class="heti-spacing"> </span></span>中，我们可以得到：</p>
<div class="arithmatex">\[
\mathbb{E}_{q_{\phi}(z_{1:T}|x)}[\log \frac{p_{\theta}(x, z_{1:T})}{q_{\phi}(z_{1:T}|x)}] = \mathbb{E}_{q_{\phi}(z_{1:T}|x)}\left[\log\frac{p(z_T)p_\theta{x|z_1}\prod_{t=2}^Tp_\theta(z_{t-1}|z_t)}{q_\phi(z_1|x)\prod_{t=2}^Tq_\phi(z_t|z_{t-1})}\right]
\]</div>
<p>当我们研究变分扩散模型时，这个目标可以进一步分解为可解释的组件。</p>
<h2 id="variational-diffusion-models">Variational Diffusion Models<a class="headerlink" href="#variational-diffusion-models" title="Permanent link">¶</a></h2>
<p>一种最简单看待<span class="heti-skip"><span class="heti-spacing"> </span>Variational Diffusion Models(VDM)<span class="heti-spacing"> </span></span>的方式是将其看作有三个关键约束的<span><span class="heti-spacing"> </span>MHVAE</span>。这三个约束是：</p>
<ul>
<li>The latent dimension is exactly equal to the data dimension</li>
<li>The structure of the latent encoder at each timestep is not learned; it is pre-defined as a linear Gaussian model. In other words, it is a Gaussian distribution centered around the output of the previous timestep</li>
<li>The Gaussian parameters of the latent encoders vary over time in such a way that the distribution of the latent at final timestep T is a standard Gaussian</li>
</ul>
<p></p><center><img src="./figures/2024-10-28-00-19-03.png" width="60%"/></center>
<p>我们用 <span class="arithmatex">\(x_t\)</span> 表示真实数据采样和潜在变量，<span class="arithmatex">\(t_0\)</span> 表示<span><span class="heti-spacing"> </span>true data sample</span>，<span class="arithmatex">\(t\in[1,T]\)</span> 表示相应的<span><span class="heti-spacing"> </span>latent</span>。<span>VDM<span class="heti-spacing"> </span></span>的先验与<span class="heti-skip"><span class="heti-spacing"> </span>HMVAE<span class="heti-spacing"> </span></span>相同，根据上面的定义可以写成：</p>
<div class="arithmatex">\[
\begin{aligned}
q(x_{1:T}|x_0) = \prod_{t=1}^T q(x_t|x_{t-1})
\end{aligned}
\]</div>
<p>根据第二个假设，我们知道每一个<span class="heti-skip"><span class="heti-spacing"> </span>latent variable<span class="heti-spacing"> </span></span>分布是以其先前的分层<span class="heti-skip"><span class="heti-spacing"> </span>latent variable<span class="heti-spacing"> </span></span>为中心（期望是<span><span class="heti-spacing"> </span>latent variable</span>）的高斯分布。每一步的<span><span class="heti-spacing"> </span>encoder</span>（加噪过程）都是固定的，均值和标准差可以是预先定义的超参数，也可以作为参数被学习。这个过程的数学表示是：</p>
<div class="arithmatex">\[
\begin{aligned}
\mu_t(x_t) = \sqrt{\alpha_t}x_{t-1}\\
\Sigma_t(x_t) = \sqrt{1-\alpha_t}I
\end{aligned}
\]</div>
<p>这个设计保证方差保持在相似的范围内，也就是说是<span><span class="heti-spacing"> </span>variance-preserving.</span>（利用参数控制）</p>
<p>这里的重点是 <span class="arithmatex">\(\alpha_t\)</span> 可能是一个可学习的参数，随着时间步的增大而变化。</p>
<p>因此，数学上可以表示为</p>
<div class="arithmatex">\[
q(x_t|x_{t-1})=\mathcal{N}(x_t;\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I)
\]</div>
<p>从第三个假设中可以知道，加噪过程的最终结果是将<span class="heti-skip"><span class="heti-spacing"> </span>latent<span class="heti-spacing"> </span></span>变为高斯噪声。那么<span class="heti-skip"><span class="heti-spacing"> </span>VDM<span class="heti-spacing"> </span></span>的联合分布可以写成：</p>
<div class="arithmatex">\[
p(x_{0:T}) = p(x_T)\prod_{t=1}^T p_\theta(x_{t-1}|x_t)
\]</div>
<p>其中 <span class="arithmatex">\(p(x_T) = \mathcal{N}(X_T;0,I)\)</span></p>
<p>这组假设描述的是图像输入随时间的稳定噪声；我们通过添加高斯噪声逐渐破坏图像，直到最终它变得与纯高斯噪声完全相同。</p>
<p>注意到我们的<span class="heti-skip"><span class="heti-spacing"> </span>encoder<span class="heti-spacing"> </span></span>不再通过 <span class="arithmatex">\(\phi\)</span> 参数化，而是完全由在每一时间步的有预定义的均值与方差的高斯分布建模。因此，在<span class="heti-skip"><span class="heti-spacing"> </span>VDM<span class="heti-spacing"> </span></span>中，我们只关注去噪过程，即 <span class="arithmatex">\(p_\theta(x_{t-1}|x_t)\)</span> 在优化<span class="heti-skip"><span class="heti-spacing"> </span>VDM<span class="heti-spacing"> </span></span>后，采样过程就很简单了，只需要迭代运行<span class="heti-skip"><span class="heti-spacing"> </span>T<span class="heti-spacing"> </span></span>个步骤的去噪转换即可。与任何的<span class="heti-skip"><span class="heti-spacing"> </span>HVAE<span class="heti-spacing"> </span></span>一样，<span>VDM<span class="heti-spacing"> </span></span>也可以通过最大化<span class="heti-skip"><span class="heti-spacing"> </span>ELBO<span class="heti-spacing"> </span></span>进行优化。</p>
<div class="arithmatex">\[
\begin{aligned}
\log p(x) &amp;\geq \mathbb{E}_{q(x_{1:T}|x_0)}[\log \frac{p(x_{0:T})}{q(x_{1:T}|x_0)}]\\
&amp;= \mathbb{E}_{q(x_{1:T}|x_0)}[\log\frac{p(x_T)\prod_{t=1}^Tp_\theta(x_{t-1}|x_t)}{\prod_{t=1}^Tq(x_t|x_{t-1})}]\\
&amp;= \mathbb{E}_{q(x_{1:T}|x_0)}[\log\frac{p(x_T)p_\theta(x_0|x_1)\prod_{t=1}^{T-1}p_\theta(x_{t}|x_{t+1})}{q(x_T|x_{T-1})\prod_{t=1}^{T-1}q(x_t|x_{t-1})}]\\
&amp;= \mathbb{E}_{q(x_{1:T}|x_0)}[\log\frac{p(x_T)p_\theta(x_0|x_1)}{q(x_T|x_{T-1})}] + \mathbb{E}_{q(x_{1:T}|x_0)}[\sum_{t=1}^{T-1}\log\frac{p_\theta(x_{t}|x_{t+1})}{q(x_t|x_{t-1})}]\\
&amp;= \mathbb{E}_{q(x_{1:T}|x_0)}[\log p_\theta(x_0|x_1)]+\mathbb{E}_{q(x_{1:T}|x_0)}[\log\frac{p(x_T)}{q(x_T|x_{T-1})}] + \sum_{t=1}^{T-1}\mathbb{E}_{q(x_{1:T}|x_0)}[\log\frac{p_\theta(x_{t}|x_{t+1})}{q(x_t|x_{t-1})}]\\
&amp;= \mathbb{E}_{q(x_{1}|x_0)}[\log p_\theta(x_0|x_1)]+\mathbb{E}_{q(x_{T-1},x_T|x_0)}[\log\frac{p(x_T)}{q(x_T|x_{T-1})}] + \sum_{t=1}^{T-1}\mathbb{E}_{q(x_{t-1},x_{t},x_{t+1}|x_0)}[\log\frac{p_\theta(x_{t}|x_{t+1})}{q(x_t|x_{t-1})}]\\
&amp;= \mathbb{E}_{q(x_{1}|x_0)}[\log p_\theta(x_0|x_1)] - \mathbb{E}_{q(x_{T-1}|x_0)}[D_{KL}(q(x_T|x_{T-1})||p(x_T))] \\
&amp; - \sum_{t=1}^{T-1}\mathbb{E}_{q(x_{t-1},x_{t+1}|x_0)}[D_{KL}(q(x_t|x_{t-1})||p_\theta(x_{t}|x_{t+1})]]
\end{aligned}
\]</div>
<p>这里的第一项是重建项，与之前类似。
第二项是先验匹配项，当最终的潜在分布与高斯先验匹配时，它会被最小化。该项不需要优化，因为它没有可训练的参数；此外，由于我们假设 T 足够大，使得最终分布呈高斯分布，因此该项实际上变为零。
第三项是一致性项。它努力使得 <span class="arithmatex">\(x_t\)</span> 的分布在前向和后向的过程中保持一致。</p>
<p></p><center><img src="./figures/2024-10-28-01-08-31.png" width="60%"/></center>
<p>由于我们必须在所有的<span class="heti-skip"><span class="heti-spacing"> </span>timestep<span class="heti-spacing"> </span></span>上进行优化，优化<span class="heti-skip"><span class="heti-spacing"> </span>VDM<span class="heti-spacing"> </span></span>的成本主要由第三项决定。</p>
<p>然而，这样得到的<span class="heti-skip"><span class="heti-spacing"> </span>ELBO<span class="heti-spacing"> </span></span>可能不是最优的，因为一致性项的计算是通过两个随机变量 <span class="arithmatex">\(x_{t-1},x_{t+1}\)</span> 来计算的，这样得到的方差可能比只用一个随机变量计算的方差要大。在累积了 <span class="arithmatex">\(T-1\)</span> 个时间步以后，得到的方差可能会很大。因此，我们可以通过引入一个额外的约束来减小这个方差。<span class="arithmatex">\(q(x_t|x_{t-1})=q(x_t|x_{t-1},x_0)\)</span>，由于马尔可夫性质，额外的条件项是多余的，那么根据贝叶斯法则，我们可以得到：</p>
<div class="arithmatex">\[
\begin{aligned}
q(x_t|x_{t-1},x_0) &amp;= \frac{q(x_{t-1}|x_t,x_0)q(x_t|x_0)}{q(x_{t-1},x_0)}
\end{aligned}
\]</div>
<p>有了这个新的约束，我们可以得到一个更好的<span><span class="heti-spacing"> </span>ELBO</span>：这中间添加的 <span class="arithmatex">\(q(x_{t-1}|x_t,x_0)\)</span> 可以消去推导过程中的一些量，最后的结果是</p>
<div class="arithmatex">\[
\begin{aligned}
&amp; \mathbb{E}_{q(x_{1}|x_0)}[\log p_\theta(x_0|x_1)] - D_{KL}(q(x_T|x_{0})||p(x_T)) \\
&amp; - \sum_{t=2}^{T}\mathbb{E}_{q(x_{t}|x_0)}[D_{KL}(q(x_{t-1}|x_{t},x_0)||p_\theta(x_{t-1}|x_{t})]]
\end{aligned}
\]</div>
<p>第二项可以重新解释为最终噪声输入的分布与标准高斯先验的接近程度
第三项可以重新解释为每个去噪步骤的一致性，我们学习所需的去噪转换步骤 <span class="arithmatex">\(p_\theta(x_{t−1}|x_t)\)</span> 作为易于处理的、真实的去噪转换步骤 <span class="arithmatex">\(q(x_{t−1}|x_t, x_0)\)</span> 的近似值。<span class="arithmatex">\(q(x_{t−1}|x_t, x_0)\)</span> 可以充当 ground truth signal。这一项当学习到的去噪声过程与真实的去噪声过程一致时，会被最小化。</p>
<p>由于同时学习编码器增加了复杂性，因此最小化任意复杂马尔可夫<span class="heti-skip"><span class="heti-spacing"> </span>HVAE<span class="heti-spacing"> </span></span>中的任意后验分布，每个<span class="heti-skip"><span class="heti-spacing"> </span>KL<span class="heti-spacing"> </span></span>散度项很难最小化，在<span class="heti-skip"><span class="heti-spacing"> </span>VDM<span class="heti-spacing"> </span></span>中我们可以利用高斯转移假设来使优化变得容易处理：</p>
<div class="arithmatex">\[
q(x_{t-1}|x_t,x_0) = \frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_{t}|x_0)}
\]</div>
<p>在这里，我们已经知道 <span class="arithmatex">\(q(x_t|x_{t-1},x_0)=\mathcal{N}(x_t;\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I)\)</span>，所以我们只需要 <span class="arithmatex">\(q(x_{t-1}|x_0)\)</span> 和 <span class="arithmatex">\(q(x_{t}|x_0)\)</span>。这里可以利用之前提到的重参数化技巧，从 <span class="arithmatex">\(x_t \sim q(x_t|x_{t-1})\)</span> 中采样</p>
<div class="arithmatex">\[
x_t = \sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}\epsilon_t
\]</div>
<p>同样的，可以从 <span class="arithmatex">\(x_{t-1} \sim q(x_{t-1}|x_{t-2})\)</span> 中采样</p>
<div class="arithmatex">\[
x_{t-1} = \sqrt{\alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_{t-1}}\epsilon_{t-1}
\]</div>
<p>因此，可以通过递归这一操作得到 <span class="arithmatex">\(x_t \sim q(x_t|x_0)\)</span></p>
<div class="arithmatex">\[
\begin{aligned}
x_t &amp;= \sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}\epsilon^*_{t-1}    \\
&amp;= \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_{t-1}}\epsilon^*_{t-2}) + \sqrt{1-\alpha_t}\epsilon^*_{t-1} \\
&amp;=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{\sqrt{\alpha_t-\alpha_t\alpha_{t-1}}^2+\sqrt{1-\alpha_t}^2}\epsilon_{t-2}\\
&amp;= \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_t\alpha_{t-1}}\epsilon_{t-2} \\
&amp;= ... \\
&amp; = \sqrt{\prod_{i=1}^{t}\alpha_i}x_0 + \sqrt{1-\prod_{i=1}^{t}\alpha_i}\epsilon_0 \\ 
&amp; = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon_0 \\
&amp; \sim \mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,\sqrt{1-\bar{\alpha}_t}I)
\end{aligned}
\]</div>
<p>中间用到了所有的噪声都是独立同分布的，利用了高斯分布的特性。</p>
<p>现在我们已经得到了前面贝叶斯形式的去噪过程所需要的全部项，现在来推导最后的去噪声过程</p>
<p></p><center><img src="./figures/2024-10-28-01-47-34.png" width="100%"/></center>
<p>因此，我们证明了在每一步中 <span class="arithmatex">\(x_{t-1}\sim q(x_{t-1}|x_t,x_0)\)</span> 呈正态分布，其中均值 <span class="arithmatex">\(\mu_q(x_t,x_0)\)</span> 是 <span class="arithmatex">\(x_t\)</span> 和 <span class="arithmatex">\(x_0\)</span> 的函数，方差 <span class="arithmatex">\(\Sigma_q(t)\)</span> 是 <span class="arithmatex">\(\alpha\)</span> 系数。这些 <span class="arithmatex">\(\alpha\)</span> 系数是已知的并且在每个时间步都是固定的；它们要么在建模为超参数时永久设置，要么被视为试图对其建模的网络的当前推理输出。</p>
<p>这里两个关键的参数是</p>
<div class="arithmatex">\[
\mu_q(x_t,x_0) = \frac{\sqrt{\alpha_t(1-\overline{\alpha_{t-1}})x_t}+\sqrt{\overline{\alpha_{t-1}}}(1-\alpha_t)x_0}{1-\overline{\alpha_t}} 
\]</div>
<div class="arithmatex">\[
\sigma_q(t)^2 = \frac{(1-\alpha_t)(1-\overline{\alpha_{t-1}})}{1-\overline{\alpha_{t}}}
\]</div>
<p>从两个正态分布的<span class="heti-skip"><span class="heti-spacing"> </span>KL<span class="heti-spacing"> </span></span>散度的变换出发：</p>
<div class="arithmatex">\[
\begin{aligned}
&amp; \argmin_\theta D_{\text{KL}}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t)) \\
&amp;= \argmin_\theta \frac{1}{2}(\log\frac{|\Sigma_{p_\theta}|}{|\Sigma_q|} - d + \text{tr}(\Sigma_{p_\theta}^{-1}\Sigma_q) + (\mu_{p_\theta}-\mu_q)^T\Sigma_{p_\theta}^{-1}(\mu_{p_\theta}-\mu_q)) \\
&amp;= \arg \min_{\theta} \frac{1}{2 \sigma_q^2(t)} \left\| \mu_\theta - \mu_q \right\|_2^2
\end{aligned}
\]</div>
<p>这里 <span class="arithmatex">\(\mu_q = \mu_q(x_t,x_0),\mu_\theta=\mu_\theta(x_t,t)\)</span></p>
<p>根据前面的推导，我们可以将 <span class="arithmatex">\(\mu_\theta(x_t,t)\)</span> 设置成下面的形式来逼近 <span class="arithmatex">\(\mu_q(x_t,x_0)\)</span></p>
<div class="arithmatex">\[
\begin{aligned}
\mu_\theta(x_t,t)=\frac{\sqrt{\alpha_t}(1-\overline{\alpha_{t-1}})x_t+\sqrt{\overline{\alpha_{t-1}}}(1-\alpha_t)\hat{x_\theta}(x_t,t)}{1-\overline{\alpha_{t}}}
\end{aligned}
\]</div>
<p>这里 <span class="arithmatex">\(\hat{x_\theta}(x_t,t)\)</span> 是由神经网络参数化的，这个神经网络的寻求从噪声图像 <span class="arithmatex">\(x_t\)</span> 和时间步骤 <span class="arithmatex">\(t\)</span> 预测 <span class="arithmatex">\(x_0\)</span></p>
<p>那么对<span class="heti-skip"><span class="heti-spacing"> </span>KL<span class="heti-spacing"> </span></span>散度的优化可以转化为</p>
<div class="arithmatex">\[
\begin{aligned}
&amp; \argmin_\theta D_{\text{KL}}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t)) \\
&amp; = \arg \min_{\theta} \frac{1}{2 \sigma_q^2(t)} \frac{\overline{\alpha_{t-1}}(1-\alpha_t)^2}{(1-\overline{\alpha_{t}})^2}[\left\|\hat{x_\theta}(x_t,t)-x_0\right\|^2_2]
\end{aligned}
\]</div>
<p>此外，最小化我们导出的<span class="heti-skip"><span class="heti-spacing"> </span>ELBO<span class="heti-spacing"> </span></span>目标在所有噪声水平上的求和项可以通过最小化所有时间步长的期望来近似：</p>
<div class="arithmatex">\[
\argmin_\theta \mathbb{E}_{t\sim U{2,T}}[\mathbb{E}_{q(x_t|x_0)}[D_{\text{KL}}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t))]]
\]</div>
<p>然后可以使用随时间步长的随机样本进行优化。</p>
<h2 id="learning-diffusion-noise-parameters">Learning Diffusion Noise parameters<a class="headerlink" href="#learning-diffusion-noise-parameters" title="Permanent link">¶</a></h2>
<p>一个可能的方式是通过以 <span class="arithmatex">\(\eta\)</span> 参数化的神经网络 <span class="arithmatex">\(\hat{\alpha}_\eta(t)\)</span> 来预测 <span class="arithmatex">\(\alpha_t\)</span>。但是为了计算 <span class="arithmatex">\(\bar{\alpha}_t\)</span>，需要在每一个<span class="heti-skip"><span class="heti-spacing"> </span>timestep<span class="heti-spacing"> </span></span>做一些推断，这是低效的。尽管可以用缓存的方式来提高效率，我们也可以从数学的方式改进公式。</p>
<p>将先前得到的 <span class="arithmatex">\(\sigma_q^2(t)\)</span> 带入目标，我们得到：</p>
<div class="arithmatex">\[
\begin{aligned}
&amp;\frac{1}{2 \sigma_q^2(t)} \frac{\overline{\alpha_{t-1}}(1-\alpha_t)^2}{(1-\overline{\alpha_{t}})^2}[\left\|\hat{x_\theta}(x_t,t)-x_0\right\|^2_2] \\
&amp;=\frac{1}{2\frac{(1-\alpha_t)(1-\overline{\alpha_{t-1}})}{1-\overline{\alpha_{t}}}}\frac{\overline{\alpha_{t-1}}(1-\alpha_t)^2}{(1-\overline{\alpha_{t}})^2}[\left\|\hat{x_\theta}(x_t,t)-x_0\right\|^2_2]\\
&amp;=\frac{1}{2}\left(\frac{\overline{\alpha_{t-1}}}{1-\overline{\alpha_{t-1}}}-\frac{\overline{\alpha_{t}}}{1-\overline{\alpha_{t}}}\right)[\left\|\hat{x_\theta}(x_t,t)-x_0\right\|^2_2]
\end{aligned}
\]</div>
<div class="admonition note snr">
<p class="admonition-title">Note</p>
<p>SNR means signal-to-noise ratio <span class="arithmatex">\(\text{SNR}=\frac{\mu^2}{\sigma^2}\)</span></p>
</div>
<p>根据前面的推导，<span class="arithmatex">\(q(x_t|x_0) \sim \mathcal{N}(x_t;\sqrt{\overline{\alpha_t}}x_0,(1-\overline{\alpha_t})I)\)</span> 那么我们就可以写出 </p>
<div class="arithmatex">\[
\text{SNR}(t) =\frac{\overline{\alpha_{t}}}{1-\overline{\alpha_{t}}}
\]</div>
<p>那么前面的公式就可以转化为</p>
<div class="arithmatex">\[
\begin{aligned}
&amp;\frac{1}{2}\left(\frac{\overline{\alpha_{t-1}}}{1-\overline{\alpha_{t-1}}}-\frac{\overline{\alpha_{t}}}{1-\overline{\alpha_{t}}}\right)[\left\|\hat{x_\theta}(x_t,t)-x_0\right\|^2_2] \\
&amp;=\frac{1}{2}(\text{SNR}(t-1)-\text{SNR}(t))[\left\|\hat{x_\theta}(x_t,t)-x_0\right\|^2_2]
\end{aligned}
\]</div>
<p>顾名思义，<span>SNR<span class="heti-spacing"> </span></span>表示原始信号与存在的噪声量之间的比率；较高的<span class="heti-skip"><span class="heti-spacing"> </span>SNR<span class="heti-spacing"> </span></span>表示较多的信号，较低的<span class="heti-skip"><span class="heti-spacing"> </span>SNR<span class="heti-spacing"> </span></span>表示较多的噪声。在扩散模型中我们要求<span class="heti-skip"><span class="heti-spacing"> </span>SNR<span class="heti-spacing"> </span></span>随着时间步<span class="heti-skip"><span class="heti-spacing"> </span>t<span class="heti-spacing"> </span></span>的增加而单调减小；这形式化了这样一个概念：受扰动的输入 <span class="arithmatex">\(x_t\)</span> 随着时间的推移变得越来越嘈杂，直到它在 <span class="arithmatex">\(t = T\)</span> 时变得与标准高斯相同。</p>
<p>根据前面的推导，我们可以使用神经网络直接参数化每个时间步的<span><span class="heti-spacing"> </span>SNR</span>，并与扩散模型一起联合学习。由于<span class="heti-skip"><span class="heti-spacing"> </span>SNR<span class="heti-spacing"> </span></span>必须随时间单调下降，我们可以将其表示为：</p>
<div class="arithmatex">\[
\text{SNR}(t) = \exp(-\omega_\eta(t))
\]</div>
<p>其中 <span class="arithmatex">\(\omega_\eta(t)\)</span> 被建模为参数为 <span class="arithmatex">\(\eta\)</span> 的单调递增神经网络。那么<span class="arithmatex">\(\overline{\alpha_t}\)</span> 有一个优雅的形式</p>
<div class="arithmatex">\[
\begin{aligned}
\overline{\alpha_t} = \text{sigmoid}(-\omega_\eta(t)) \\
1-\overline{\alpha_t} = \text{sigmoid}(\omega_\eta(t))
\end{aligned}
\]</div>
<h2 id="three-equivalent-interpretations">Three Equivalent Interpretations<a class="headerlink" href="#three-equivalent-interpretations" title="Permanent link">¶</a></h2>
<p>正如我们之前证明的，可以通过简单地学习神经网络来训练变分扩散模型，以从任意噪声版本 <span class="arithmatex">\(x_t\)</span> 及其时间索引 <span class="arithmatex">\(t\)</span> 预测原始自然图像 <span class="arithmatex">\(x_0\)</span>。而 <span class="arithmatex">\(x_0\)</span> 有另外两种参数化方法，可以推出<span class="heti-skip"><span class="heti-spacing"> </span>VDM<span class="heti-spacing"> </span></span>的两个等价解释。</p>
<p>在前面的推导中，利用重参数方法，我们可以用 <span class="arithmatex">\(x_t\)</span> 和噪声表示 <span class="arithmatex">\(x_0\)</span>，即</p>
<div class="arithmatex">\[
x_0 = \frac{x_t - \sqrt{1-\overline{\alpha_t}}\epsilon_0}{\sqrt{\overline{\alpha_t}}}
\]</div>
<p>那么将前面方差中的 <span class="arithmatex">\(x_0\)</span> 用上面的公式代入，我们可以得到</p>
<div class="arithmatex">\[
\mu_q(x_t,x_0) = \frac{1}{\sqrt{\alpha_t}}x_t - \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha_{t}}}\sqrt{\alpha_t}}\epsilon_0
\]</div>
<p>因此，我们可以设置<span class="heti-skip"><span class="heti-spacing"> </span>approximate denoising transition mean<span class="heti-spacing"> </span></span>为</p>
<div class="arithmatex">\[
\mu_\theta(x_t,t) = \frac{1}{\sqrt{\alpha_t}}x_t - \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha_{t}}}\sqrt{\alpha_t}}\hat{x_\theta}(x_t,t)
\]</div>
<p>相应的优化问题转化为：</p>
<div class="arithmatex">\[
\begin{aligned}
\argmin_\theta D_{\text{KL}}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t)) \\
= \argmin_{\theta} \frac{1}{2 \sigma_q^2(t)} \frac{(1 - \alpha_t)^2}{(1 - \bar{\alpha}_t) \alpha_t} \left\| \epsilon_0 - \hat{\epsilon}_\theta(x_t, t) \right\|_2^2
\end{aligned}
\]</div>
<p>这里 <span class="arithmatex">\(\hat{\epsilon}_\theta(x_t, t)\)</span>是一个神经网络，它学习预测源噪声 <span class="arithmatex">\(\epsilon_0 \sim \mathcal{N}(\epsilon;0,I)\)</span> 从而根据 <span class="arithmatex">\(x_0\)</span> 确定 <span class="arithmatex">\(x_t\)</span>。这样我们就证明了通过预测原始图像 <span class="arithmatex">\(x_0\)</span> 来学习<span class="heti-skip"><span class="heti-spacing"> </span>VDM<span class="heti-spacing"> </span></span>等价于学习预测噪声。</p>
<p>第三种等价形式需要利用<span><span class="heti-spacing"> </span>Tweedie</span>’s Formula.</p>
<div class="admonition note">
<p class="admonition-title">Tweedie's Formula</p>
<p>给定从中抽取的样本，指数族分布的真实平均值可以通过样本的最大似然估计（也称为经验平均值）加上一些涉及估计分数的校正项来估计。如果只有一个观察样本，经验平均值就是样本本身。它通常用于减轻样本偏差；如果观察到的样本全部位于基础分布的一端，则负分数会变大，并将样本的朴素最大似然估计校正为真实平均值。
数学上的表示是，对高斯变量 <span class="arithmatex">\(z\sim \mathcal{N}(z;\mu_z,\Sigma_z)\)</span>，Tweedie’s Formula states that:</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{E}[\mu_z|z] = z + \Sigma_z\nabla_z\log p(z)
\end{aligned}
\]</div>
</div>
<p>这里的情况中我们有：</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{E}[\mu_{x_t}|x_t] &amp;= x_t + (1-\overline{\alpha_t})\nabla_{x_t}\log p(x_t) \\
\end{aligned}
\]</div>
<p>为了简化，我们把 <span class="arithmatex">\(\nabla_{x_t}\log p(x_t)\)</span> 记为 <span class="arithmatex">\(\nabla\log p(x_t)\)</span>。</p>
<p>根据前面的推导我们有</p>
<div class="arithmatex">\[
q(x_t|x_0) = \mathcal{N}(x_t;\sqrt{\overline{\alpha_t}}x_0,\sqrt{1-\overline{\alpha_t}}I)
\]</div>
<p>根据<span class="heti-skip"><span class="heti-spacing"> </span>Tweedie<span class="heti-spacing"> </span></span>公式，<span class="arithmatex">\(x_t\)</span> 生成的真实均值的最佳估计为：<span class="arithmatex">\(\mu_{x_t} = \sqrt{\overline{\alpha_t}}x_0\)</span></p>
<p>带入上面的公式，我们可以得到</p>
<div class="arithmatex">\[
x_0 = \frac{x_t + \sqrt{1-\overline{\alpha_t}}\nabla \log p(x_t)}{\sqrt{\overline{\alpha_t}}}
\]</div>
<p>这里我们就得到了 <span class="arithmatex">\(x_0\)</span> 的另一种表示，将其带入实际去噪过程的均值 <span class="arithmatex">\(\mu_q(x_t,x_0)\)</span> 中，我们可以得到<span><span class="heti-spacing"> </span>:</span></p>
<div class="arithmatex">\[
\begin{aligned}
\mu_q(x_t,x_0) &amp;= \frac{1}{\sqrt{\alpha_t}}x_t + \frac{1-\alpha_t}{\sqrt{\alpha_t}}\nabla \log p(x_t)
\end{aligned}
\]</div>
<p>接着和之前同样的步骤，我们可以设置近似去噪过程的均值为</p>
<div class="arithmatex">\[
\begin{aligned}
\mu_\theta(x_t,t) &amp;= \frac{1}{\sqrt{\alpha_t}}x_t + \frac{1-\alpha_t}{\sqrt{\alpha_t}}s_\theta(x_t,t)
\end{aligned}
\]</div>
<p>这里 <span class="arithmatex">\(s_\theta(x_t,t)\)</span> 是一个神经网络，它学习预测 <span class="arithmatex">\(\nabla_{x_t}\log p(x_t)v\)</span> 的梯度。相应的优化问题转化为：</p>
<div class="arithmatex">\[
\begin{aligned}
\argmin_\theta D_{\text{KL}}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t)) \\
= \argmin_{\theta} \frac{1}{2 \sigma_q^2(t)} \frac{(1 - \alpha_t)^2}{\alpha_t} \left\| s_\theta(x_t,t) - \nabla \log p(x_t) \right\|_2^2
\end{aligned}
\]</div>
<p>这里的<span class="heti-skip"><span class="heti-spacing"> </span>score function<span class="heti-spacing"> </span></span><span class="arithmatex">\(\nabla \log p(x_t)\)</span>与噪声的非常像，具体来说有</p>
<div class="arithmatex">\[
\nabla \log p(x_t) = - \frac{1}{\sqrt{1-\overline{\alpha_t}}}\epsilon_0
\]</div>
<p>事实证明，这两项之间存在一个随时间变化的常数因子！得分函数衡量如何在数据空间中移动以最大化对数概率。直观上，由于源噪声被添加到自然图像中以破坏它，因此沿相反方向移动会对图像进行“去噪”，并且将是增加后续对数概率的最佳更新。上面的数学证明已经说明了这一点。</p>
<p>综合以上，我们有了三个等价的方式来优化一个<span><span class="heti-spacing"> </span>VDM</span>：预测原始图像，预测噪声，预测图像在任意噪声水平下的得分函数。这三种方式都可以通过神经网络来实现。</p>
<h2 id="score-based-generative-modeling">Score-based Generative Modeling<a class="headerlink" href="#score-based-generative-modeling" title="Permanent link">¶</a></h2>
<p>可以在变分扩散模型和基于分数的生成模型之间建立了明确的联系，无论是在训练目标还是抽样过程中。利用朗之万动力学的思想。</p>
<p>首先任意灵活的概率分布可以写成以下形式：</p>
<div class="arithmatex">\[
p_\theta(x) = \frac{1}{Z_\theta}e^{-f_\theta(x)}
\]</div>
<p>其中 <span class="arithmatex">\(f_\theta(x)\)</span> 是一个任意自由的参数化的势能函数，<span class="arithmatex">\(Z_\theta\)</span> 是归一化常数。学习这样一个分布的方法可以是最大似然估计，但是这样的方法求解归一化系数依赖于 <span class="arithmatex">\(Z_\theta=\int e^{-f_\theta(x)}dx\)</span>，这对复杂的 <span class="arithmatex">\(f_\theta(x)\)</span> 是很困难的。</p>
<p>避免建模或计算归一化参数的方法是用神经网络 <span class="arithmatex">\(s_\theta(x)\)</span> 来近似得分函数 <span class="arithmatex">\(\nabla \log p_\theta(x)\)</span>，这里的推导是</p>
<div class="arithmatex">\[
\begin{aligned}
\nabla_x \log p_\theta(x) =&amp; \nabla_x \log(\frac{1}{Z_\theta}e^{-f_\theta(x)}) \\ =&amp; \nabla_x \log \frac{1}{Z_\theta} + \nabla_x \log e^{-f_\theta(x)} \\ =&amp; -\nabla_x \log Z_\theta - \nabla_x f_\theta(x) \\ =&amp; -\nabla_x f_\theta(x) \\
&amp; \approx s_\theta(x)
\end{aligned}
\]</div>
<p>可以通过使用真实得分函数最小化费舍尔散度来优化得分模型</p>
<div class="arithmatex">\[
\mathbb{E}_{p(x)}[\left\|s_\theta(x)-\nabla\log p(x)\right\|_2^2]
\]</div>
<p><span>Score function<span class="heti-spacing"> </span></span>意义：对于每个 <span class="arithmatex">\(x\)</span>，取其对数似然相对于 <span class="arithmatex">\(x\)</span> 的梯度本质上描述了数据空间中移动的方向，以进一步增加其似然。直观上，得分函数定义了数据 <span class="arithmatex">\(x\)</span> 所在的整个空间上的向量场，指向众数。然后，通过学习真实数据分布的得分函数，我们可以通过从同一空间中的任意点开始并迭代地跟踪得分直到达到众数来生成样本。该采样过程称为朗之万动力学，在数学上描述为</p>
<div class="arithmatex">\[
x_{i+1} = x_i + c\nabla\log p(x_i) + \sqrt{2c}\epsilon_i
\]</div>
<p>其中 <span class="arithmatex">\(x_0\)</span> 是从先验分布（比如均匀分布）中随机采样的 <span class="arithmatex">\(\epsilon_i\)</span> 是随机噪声，来确保生成的样本不会总是<span><span class="heti-spacing"> </span>collapse onto a mode</span>，, but hover around it for diversity。此外，由于学习的得分函数是确定性的，因此涉及噪声项的采样增加了生成过程的随机性，使我们能够避免确定性轨迹。当从位于多个模式之间的位置初始化采样时​​，这特别有用。</p>
<p></p><center><img src="./figures/2024-10-29-01-20-51.png" width="80%"/></center>
<p>需要注意的是对于复杂的分布，我们没法直接使用<span class="heti-skip"><span class="heti-spacing"> </span>ground-truth<span class="heti-spacing"> </span></span>的得分函数，好在已经衍生出称为<span class="heti-skip"><span class="heti-spacing"> </span>score matching<span class="heti-spacing"> </span></span>的方法，可以在不知道真实分数的情况下最小化费舍尔散度，并且可以通过随机梯度下降进行优化。</p>
<p>总的来说，学习将分布表示为得分函数，并使用它通过马尔可夫链蒙特卡罗技术（例如朗之万动力学）生成样本，被称为基于得分的生成建模。</p>
<p><span>vanilla score matching<span class="heti-spacing"> </span></span>存在三个主要的问题</p>
<ol>
<li>当<span class="heti-skip"><span class="heti-spacing"> </span>x<span class="heti-spacing"> </span></span>位于高维空间中的低维流形上时，得分函数是不明确的。这可以从数学上看出；不在低维流形上的所有点的概率为零，其对数未定义。当尝试学习自然图像的生成模型时，这尤其不方便，因为众所周知，自然图像位于整个环境空间的低维流形上。</li>
<li>通过普通分数匹配训练的估计分数函数在低密度区域将不准确。因为它是对<span class="heti-skip"><span class="heti-spacing"> </span>p(x)<span class="heti-spacing"> </span></span>的期望，并且对其样本进行显式训练，所以模型将不会收到很少见或未见示例的准确学习信号。这是有问题的，因为我们的采样策略涉及从高维空间中的随机位置（很可能是随机噪声）开始，并根据学习的得分函数移动。由于我们遵循噪声或不准确的分数估计，因此最终生成的样本也可能不是最佳的，或者需要更多的迭代才能收敛到准确的输出。</li>
<li><span>Langevin<span class="heti-spacing"> </span></span>动态采样可能不会混合，即使它是使用地面实况分数执行的。假设真实的数据分布是两个不相交分布的混合<span><span class="heti-spacing"> </span>:</span></li>
</ol>
<div class="arithmatex">\[
p(x) = c_1p_1(x) + c_2p_2(x)
\]</div>
<p>当计算分数时，这些混合系数就会丢失，因为对数运算将系数从分布中分离出来，而梯度运算将其归零<span><span class="heti-spacing"> </span>.</span></p>
<p>解决方案：通过向数据添加多个级别的高斯噪声可以同时解决这三个缺点。首先，由于高斯噪声分布​​的支持是整个空间，扰动的数据样本将不再局限于低维流形。其次，添加大的高斯噪声将增加数据分布中每种模式覆盖的区域，在低密度区域添加更多的训练信号。最后，添加多个级别的高斯噪声并增加方差将产生尊重地面实况混合系数的中间分布。</p>
<p>形式上，我们可以选择噪声水平的正序列 <span class="arithmatex">\(\{\sigma_t\}^T_{t=1}\)</span> 并定义逐渐扰动的数据分布序列：</p>
<div class="arithmatex">\[
p_{\sigma_t}(x) = \int p(x) \mathcal{N}(x_t;x,\sigma_t^2I)dx
\]</div>
<p>然后，使用分数匹配来学习神经网络 <span class="arithmatex">\(s_\theta(x,t)\)</span>，以同时学习所有噪声级别的分数函数：</p>
<div class="arithmatex">\[
\argmin_{\theta} \sum_{t=1}^T \lambda(t) \mathbb{E}_{p_{\sigma_t}(x_t)} \left[ \left\| s_{\theta}(x, t) - \nabla \log p_{\sigma_t}(x_t) \right\|_2^2 \right]
\]</div>
<p>其中 <span class="arithmatex">\(\lambda(t)\)</span> 是一个以噪声水平<span class="heti-skip"><span class="heti-spacing"> </span>t<span class="heti-spacing"> </span></span>为条件的正加权函数<span class="heti-skip"><span class="heti-spacing"> </span>.<span class="heti-spacing"> </span></span>此外，作者提出将退火朗之万动力学采样作为一种生成过程，其中通过按顺序对每个 <span class="arithmatex">\(t = T, T −1, ..., 2, 1\)</span> 运行朗之万动力学来生成样本。初始化是从一些固定的先验（例如均匀）中选择的，并且每个后续采样步骤都从先前模拟的最终样本开始。因为噪声水平随着时间步长<span class="heti-skip"><span class="heti-spacing"> </span>t<span class="heti-spacing"> </span></span>稳定下降，并且我们随着时间的推移减小步长，所以样本最终收敛到真模式。这直接类似于变分扩散模型的马尔可夫<span class="heti-skip"><span class="heti-spacing"> </span>HVAE<span class="heti-spacing"> </span></span>解释中执行的采样过程，其中随机初始化的数据向量随着噪声水平的降低而迭代细化。</p>
<p>因此，我们在变分扩散模型和基于分数的生成模型之间建立了明确的联系，无论是在训练目标还是抽样程序上。</p>
<p>一个问题是如何自然地将扩散模型推广到无限数量的时间步长。在<span class="heti-skip"><span class="heti-spacing"> </span>MHVAE<span class="heti-spacing"> </span></span>观点下，这可以解释为将层次结构的数量扩展到无穷大 <span class="arithmatex">\(T\rightarrow\infty\)</span>。从等效的基于分数的生成模型角度来表示这一点会更清楚；在无限数量的噪声尺度下，图像在连续时间内的扰动可以表示为随机过程，因此可以用随机微分方程（SDE）来描述。然后通过反转<span class="heti-skip"><span class="heti-spacing"> </span>SDE<span class="heti-spacing"> </span></span>来执行采样，这自然需要估计每个连续值噪声级别的得分函 。 <span>SDE<span class="heti-spacing"> </span></span>的不同参数化本质上描述了随时间变化的不同扰动方案，从而实现了噪声过程的灵活建模。</p>
<h2 id="guidance">Guidance<a class="headerlink" href="#guidance" title="Permanent link">¶</a></h2>
<p><span>guidance<span class="heti-spacing"> </span></span>的目标是控制生成的数据，添加一些自己的条件，数学上就是 <span class="arithmatex">\(p(x|y)\)</span> 这样的条件概率。</p>
<p>添加条件信息的一种自然方法是在每次迭代时与时间步信息一起添加。形式是：</p>
<div class="arithmatex">\[
p(x_{0:T}|y) = p(x_T)\prod_{t=1}^Tp_\theta(x_{t-1}|x_t,y)
\]</div>
<p>这里的<span class="heti-skip"><span class="heti-spacing"> </span>y<span class="heti-spacing"> </span></span>可以是图像<span class="heti-skip"><span class="heti-spacing"> </span>-<span class="heti-spacing"> </span></span>文本生成中的文本编码，<span>or a low-resolution image in super-resolution tasks.<span class="heti-spacing"> </span></span>因此，我们可以通过预测三个等价形式来学习<span class="heti-skip"><span class="heti-spacing"> </span>VDM<span class="heti-spacing"> </span></span>的核心神经网络对每个所需的解释和实现。</p>
<p>这种方式的问题是以这种方式训练的条件扩散模型可能会学会忽略或淡化任何给定的条件信息<heti-adjacent class="heti-adjacent-half">。</heti-adjacent>（训练的时候就有这些条件）因此，提出了<span class="heti-skip"><span class="heti-spacing"> </span>Guidance<span class="heti-spacing"> </span></span>作为一种更明确地控制模型赋予调节信息的权重的方法，但以样本多样性为代价。两种最流行的指导形式被称为<span class="heti-skip"><span class="heti-spacing"> </span>classifier guidance<span class="heti-spacing"> </span></span>和<span><span class="heti-spacing"> </span>classier-free guidance</span>。</p>
<h3 id="classifier-guidance">Classifier Guidance<a class="headerlink" href="#classifier-guidance" title="Permanent link">¶</a></h3>
<p>我们的目标是学习到 <span class="arithmatex">\(\nabla \log p(x_t|y)\)</span> 在任意的噪声时间步中</p>
<div class="arithmatex">\[
\begin{aligned}
\nabla \log p(x_t|y) = \nabla \log p(x_t) + \nabla \log p(y|x_t)
\end{aligned}
\]</div>
<p>这是由贝叶斯法则得到的，其中 <span class="arithmatex">\(\nabla \log p(y)\)</span> 在对 <span class="arithmatex">\(x_t\)</span> 的梯度下为<span><span class="heti-spacing"> </span>0</span>。上面的两项分别是无条件的得分函数与分类器 <span class="arithmatex">\(p(y|x_t)\)</span> 的对抗梯度。在<span class="heti-skip"><span class="heti-spacing"> </span>Classifier Guidance<span class="heti-spacing"> </span></span>中，无条件扩散模型的分数是按照先前导出的方式学习的，同时分类器接受任意噪声<span class="heti-skip"><span class="heti-spacing"> </span>xt<span class="heti-spacing"> </span></span>并尝试预测条件信息<span><span class="heti-spacing"> </span>y</span>。然后，在采样过程中，用于退火朗之万动力学的总体条件得分函数被计算为无条件得分函数与噪声分类器的对抗梯度之和。为了提升控制的精细度，引入了超参数 <span class="arithmatex">\(\gamma\)</span>，用于控制分类器梯度的权重。</p>
<div class="arithmatex">\[
\begin{aligned}
\nabla \log p(x_t|y) = \nabla \log p(x_t) + \gamma\nabla \log p(y|x_t)
\end{aligned}
\]</div>
<p>直观上，当<span class="heti-skip"><span class="heti-spacing"> </span>$ \gamma= 0$<span class="heti-spacing"> </span></span>时，条件扩散模型学会完全忽略条件信息，而当 <span class="arithmatex">\(\gamma\)</span> 较大时，条件扩散模型学会生成严重遵循条件信息的样本。这会导致样本多样性下降，因为它只会产生易于重新生成所提供的条件信息的数据。</p>
<p><span>classifier guidance<span class="heti-spacing"> </span></span>的一个值得注意的缺点是它依赖于单独学习的分类器。由于分类器必须处理任意噪声输入，而大多数现有的预训练分类模型并未对此进行优化，因此必须与扩散模型一起进行临时学习。</p>
<h3 id="classifier-free-guidance">Classifier-Free Guidance<a class="headerlink" href="#classifier-free-guidance" title="Permanent link">¶</a></h3>
<p>在这里，学者放弃了单独分类器模型的训练，转而采用无条件扩散模型和条件扩散模型。首先</p>
<div class="arithmatex">\[
\nabla \log p(y|x_t) = \nabla \log p(x_t|y) - \nabla \log p(x_t)
\]</div>
<p>然后在先前超参约束的基础上，我们得到</p>
<div class="arithmatex">\[
\nabla \log p(x_t|y) = \gamma \nabla \log p(x_t|y) + (1-\gamma)\nabla \log p(x_t)
\]</div>
<p>这里的 <span class="arithmatex">\(\gamma\)</span> 是一个超参数，用于控制条件信息的权重，通俗地说就是对条件信息有多依赖。</p>
<p>当 <span class="arithmatex">\(\gamma = 0\)</span> 时，学习到的条件模型完全忽略条件并学习无条件扩散模型。
当 <span class="arithmatex">\(\gamma = 1\)</span> 时，模型无需指导即可显式学习普通条件分布。
当 <span class="arithmatex">\(\gamma &gt; 1\)</span> 时，扩散模型不仅优先考虑条件得分函数，而且会向远离无条件得分函数的方向移动。</p>
<p>换句话说，它降低了生成不使用条件信息的样本的概率，有利于明确使用条件信息的样本。这也会降低样本多样性，但代价是生成准确匹配调节信息的样本<span><span class="heti-spacing"> </span>.</span></p>
<p>由于学习两个单独的扩散模型的成本很高，因此我们可以将条件扩散模型和无条件扩散模型一起学习为单个条件模型；可以通过用固定常数值（例如零）替换条件信息来查询无条件扩散模型。这本质上是对条件信息执行随机丢失。无分类器指导很优雅，因为它使我们能够更好地控制条件生成过程，同时除了训练奇异扩散模型之外不需要任何其他操作。</p>
<h2 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">¶</a></h2>
<p>首先，我们推导变分扩散模型作为马尔可夫分层变分自动编码器的特例，其中三个关键假设使得<span class="heti-skip"><span class="heti-spacing"> </span>ELBO<span class="heti-spacing"> </span></span>能够进行易于处理的计算和可扩展的优化。</p>
<p>然后，我们证明优化<span class="heti-skip"><span class="heti-spacing"> </span>VDM<span class="heti-spacing"> </span></span>归结为学习神经网络来预测三个潜在目标之一：来自任意噪声的原始源图像、来自任意任意噪声图像的原始源噪声，或者噪声噪声的得分函数任意噪声水平下的图像。</p>
<p>然后，我们更深入地探讨学习得分函数的含义，并将其与基于得分的生成建模的角度明确联系起来。</p>
<p>最后，我们介绍如何使用扩散模型学习条件分布。</p>
<p>当然，<span>diffusion<span class="heti-spacing"> </span></span>模型仍然有一些缺陷：</p>
<ul>
<li>首先往信息中加噪声不太可能是人类自然地建模和生成数据的方式</li>
<li><span>VDM<span class="heti-spacing"> </span></span>不产生可解释的潜在变量。 <span>VAE<span class="heti-spacing"> </span></span>希望通过编码器的优化来学习结构化的潜在空间，而在<span class="heti-skip"><span class="heti-spacing"> </span>VDM<span class="heti-spacing"> </span></span>中，每个时间步的编码器已经作为线性高斯模型给出，无法灵活优化。因此，中间潜在变量被限制为原始输入的噪声版本。</li>
<li>潜在的维度被限制为与原始输入相同的维度，这进一步阻碍了学习有意义的、压缩的潜在结构的努力。</li>
<li>采样是一个昂贵的过程，因为必须在两种公式下运行多个去噪步骤。回想一下，限制之一是选择足够多的时间步<span><span class="heti-spacing"> </span>T</span>，以确保最终的潜伏完全是高斯噪声；在采样过程中，我们必须迭代所有这些时间步来生成样本。</li>
</ul>
<h2 id="__comments">评论</h2>
<!-- Insert generated snippet here -->
<script async="" crossorigin="anonymous" data-category="General" data-category-id="DIC_kwDONG-xF84Cj1fb" data-emit-metadata="1" data-input-position="top" data-lang="zh-CN" data-mapping="pathname" data-reactions-enabled="1" data-repo="626lt/note" data-repo-id="R_kgDONG-xFw" data-strict="0" data-theme="light" src="https://giscus.app/client.js">
</script>
<!-- Synchronize Giscus theme with palette -->
<script>
    var giscus = document.querySelector("script[src*=giscus]")

    // Set palette on initial load
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
      var theme = palette.color.scheme === "slate"
        ? "transparent_dark"
        : "light"

      // Instruct Giscus to set theme
      giscus.setAttribute("data-theme", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener("DOMContentLoaded", function() {
      var ref = document.querySelector("[data-md-component=palette]")
      ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate"
            ? "transparent_dark"
            : "light"

          // Instruct Giscus to change theme
          var frame = document.querySelector(".giscus-frame")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
          )
        }
      })
    })
  </script>
</article>
</div>
</div>
<a class="md-top md-icon" data-md-component="top" hidden="" href="#">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"></path></svg>
            回到页面顶部
          </a>
</main>
<footer class="md-footer">
<nav aria-label="页脚" class="md-footer__inner md-grid">
<a aria-label="上一页: Index" class="md-footer__link md-footer__link--prev" href="../" rel="prev">
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</div>
<div class="md-footer__title">
<div class="md-ellipsis">
<span class="md-footer__direction">
                  上一页
                </span>
                Index
              </div>
</div>
</a>
<a aria-label="下一页: Consistency Models" class="md-footer__link md-footer__link--next" href="../Consistency%20Models/" rel="next">
<div class="md-footer__title">
<div class="md-ellipsis">
<span class="md-footer__direction">
                  下一页
                </span>
                Consistency Models
              </div>
</div>
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"></path></svg>
</div>
</a>
</nav>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
<div class="md-copyright__highlight">
      Copyright © 2023-2024 <a href="https://github.com/626lt" rel="noopener" target="_blank">626lt</a>
</div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy", "content.code.annotate", "navigation.tracking", "navigation.tabs", "navigation.indexes", "navigation.top", "navigation.footer"], "search": "../../assets/javascripts/workers/search.e5c33ebb.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
<script src="../../assets/javascripts/bundle.ba449ae6.min.js"></script>
<script src="https://cdn.tonycrane.cc/utils/katex.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script src="../../js/katex.js"></script>
<script src="../../js/heti.js"></script>
</body>
</html>